{"cells":[{"metadata":{"colab_type":"text","id":"azPfc2AZQ_gI"},"cell_type":"markdown","source":"Сейчас мое ядро выглядит таким образом:"},{"metadata":{"colab":{},"colab_type":"code","id":"k0OgPJLiQ_gg","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport re\nimport numpy as np\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","execution_count":null,"outputs":[]},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":142},"colab_type":"code","id":"H2in5uGIQ_gy","outputId":"faea1b0b-fd60-4f72-b9de-8ca264bbf211","trusted":true},"cell_type":"code","source":"# Load in the train and test datasets\ntrain = pd.read_csv('/kaggle/input/titanic/train.csv')\ntest = pd.read_csv('/kaggle/input/titanic/test.csv')\n\n# Store our passenger ID for easy access\nPassengerId = test['PassengerId']\n\ntrain.head(3)","execution_count":null,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"9EECm9PHQ_g-","trusted":true},"cell_type":"code","source":"full_data = [train, test]\n\n# Gives the length of the name\ntrain['Name_length'] = train['Name'].apply(len)\ntest['Name_length'] = test['Name'].apply(len)\n# Feature that tells whether a passenger had a cabin on the Titanic\ntrain['Has_Cabin'] = train[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\ntest['Has_Cabin'] = test[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\n\n# Create new feature FamilySize as a combination of SibSp and Parch\nfor dataset in full_data:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n# Create new feature IsAlone from FamilySize\nfor dataset in full_data:\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n# Remove all NULLS in the Embarked column\nfor dataset in full_data:\n    dataset['Embarked'] = dataset['Embarked'].fillna('S')\n# Remove all NULLS in the Fare column and create a new feature CategoricalFare\nfor dataset in full_data:\n    dataset['Fare'] = dataset['Fare'].fillna(train['Fare'].median())\ntrain['CategoricalFare'] = pd.qcut(train['Fare'], 4)\n# Create a New feature CategoricalAge\nfor dataset in full_data:\n    age_avg = dataset['Age'].mean()\n    age_std = dataset['Age'].std()\n    age_null_count = dataset['Age'].isnull().sum()\n    age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)\n    dataset['Age'][np.isnan(dataset['Age'])] = age_null_random_list\n    dataset['Age'] = dataset['Age'].astype(int)\ntrain['CategoricalAge'] = pd.cut(train['Age'], 5)\n# Define function to extract titles from passenger names\ndef get_title(name):\n    title_search = re.search(' ([A-Za-z]+)\\.', name)\n    # If the title exists, extract and return it.\n    if title_search:\n        return title_search.group(1)\n    return \"\"\n# Create a new feature Title, containing the titles of passenger names\nfor dataset in full_data:\n    dataset['Title'] = dataset['Name'].apply(get_title)\n# Group all non-common titles into one single grouping \"Rare\"\nfor dataset in full_data:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n\nfor dataset in full_data:\n    # Mapping Sex\n    dataset['Sex'] = dataset['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\n    \n    # Mapping titles\n    title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n    \n    # Mapping Embarked\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n    \n    # Mapping Fare\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)\n    \n    # Mapping Age\n    dataset.loc[ dataset['Age'] <= 16, 'Age'] \t\t\t\t\t       = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[ dataset['Age'] > 64, 'Age'] = 4","execution_count":null,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"VAn5VOloQ_hF","trusted":true},"cell_type":"code","source":"# Feature selection\ndrop_elements = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp']\ntrain = train.drop(drop_elements, axis = 1)\ntrain = train.drop(['CategoricalAge', 'CategoricalFare'], axis = 1)\ntest  = test.drop(drop_elements, axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":142},"colab_type":"code","id":"leesH9piQ_hP","outputId":"286ab190-e749-4001-daf2-7df69cd88dbc","trusted":true},"cell_type":"code","source":"train.head(3)","execution_count":null,"outputs":[]},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":795},"colab_type":"code","id":"thB1MhnAQ_hU","outputId":"cfb08f30-0f05-493e-a8c5-304ec1c87acf","trusted":true},"cell_type":"code","source":"colormap = plt.cm.RdBu\nplt.figure(figsize=(14,12))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(train.astype(float).corr(),linewidths=0.1,vmax=1.0, \n            square=True, cmap=colormap, linecolor='white', annot=True)","execution_count":null,"outputs":[]},{"metadata":{"colab_type":"text","id":"YsmT2x2wQ_hk"},"cell_type":"markdown","source":"Итак, у нас получились два датасета с новыми признаками. Теперь приступим к построению модели.\n\n### Построение модели\n\n### 1.\n\nВоспользуюсь алгоритмом стекинга. В качестве базовых алгоритмов использую RandomForestClassifier, SVC, GradientBoostingClassifier и LinearRegression; в качестве мета-алгоритма - XGBoost.\n\nРазделю данные train на тренировочную и валидационную выборки с random_state=17 и параметром разбиения test_size=.3 (в качестве целевой переменной возьму столбец Survived, а в качестве признаков - все остальные столбцы).\n\nНиже приведены параметры для каждого из базовых алгоритмов, которые необходимо настроить на 5-кратной кросс-валидации с помощью GridSearchCV:"},{"metadata":{"colab":{},"colab_type":"code","id":"dbJZVqjWQ_hm","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import (GridSearchCV,\n                                     train_test_split,\n                                     StratifiedKFold)\n\nfrom sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\n\nfrom xgboost import XGBClassifier\n\n# параметры базовых алгоритмов\ngbc_params = {'learning_rate': np.arange(0.1, 0.6, 0.1)} # GradientBoostingClassifier\n\nrfc_params = {'n_estimators': range(10, 100, 10), # RandomForestClassifier\n              'min_samples_leaf': range(1, 5)}\n\nsvc_params = {'kernel': ['linear', 'rbf'], # SVC\n              'C': np.arange(0.1, 1, 0.2),\n             'gamma': np.logspace(-5, 2, num=8), \n              'degree': np.arange(2, 8, 1)}\n\nlr_params = {'C': np.arange(0.5, 1, 0.1),\n            'solver':['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}\n\nskf = StratifiedKFold(n_splits=5, random_state=17)","execution_count":null,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"n9jsf8wWBgGJ","trusted":true},"cell_type":"code","source":"targets = train.Survived\ndata    = train.drop(columns='Survived')\n\nx_train, x_test, y_train, y_test = train_test_split(data, \n                                                    targets,\n                                                    test_size=0.3,\n                                                    random_state=17)\n\nrfc = RandomForestClassifier(random_state=17)\nsvc = SVC(random_state=17)\ngbc = GradientBoostingClassifier(random_state=17)\nlr  = LogisticRegression(random_state=17)\n\nmeta = XGBClassifier()","execution_count":null,"outputs":[]},{"metadata":{"colab_type":"text","id":"aHK2wufHQ_hr"},"cell_type":"markdown","source":"### 2.\n1. Определю объект GridSearchCV для всех приведенных параметров каждого алгоритма (в гиперпараметрах алгоритма при его определении, укажу random_state=17). Параметр cv устанавливаю равным skf.\n\n2. Обучу каждый из объектов из 1-го пункта на получившейся при разбиении тренировочной выборке. Выведу лучшее сочетание параметров для каждого из алгоритмов.\n\n3. Для каждого обученного алгоритма получу предсказания на валидационных данных и выведу метрику качества, которая соответствует метрике оценки соревнования."},{"metadata":{"colab":{},"colab_type":"code","id":"w_PrrRZpQ_ht","trusted":true},"cell_type":"code","source":"# Определю объект GridSearchCV для всех приведенных параметров каждого алгоритма (в гиперпараметрах алгоритма при \n# его определении, укажу random_state=17). Параметр cv устанавливаю равным skf.\n\nrfc_model = GridSearchCV(rfc, rfc_params, cv=skf)\nsvc_model = GridSearchCV(svc, svc_params, cv=skf)\ngbc_model = GridSearchCV(gbc, gbc_params, cv=skf)\nlr_model = GridSearchCV(lr, lr_params, cv=skf)","execution_count":null,"outputs":[]},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":85},"colab_type":"code","id":"YFqsL0a4BgGW","outputId":"5556147f-d84d-4f93-a9b6-5f0bf2fddd17","trusted":true},"cell_type":"code","source":"# 2.Обучу каждый из объектов из 1-го пункта на получившейся при разбиении тренировочной выборке.\nrfc_model_fit = rfc_model.fit(x_train, y_train)\nsvc_model_fit = svc_model.fit(x_train, y_train)\ngbc_model_fit = gbc_model.fit(x_train, y_train)\nlr_model_fit = lr_model.fit(x_train, y_train)\n\n# Выведу лучшее сочетание параметров для каждого из алгоритмов\nrfc_best_param = rfc_model.best_params_\nprint(f'Лучшее сочетание параметров для алгоритма RandomForestClassifier = {rfc_best_param}')\n\nsvc_best_param = svc_model.best_params_\nprint(f'Лучшее сочетание параметров для алгоритма SVC = {svc_best_param}')\n\ngbc_best_param = gbc_model.best_params_\nprint(f'Лучшее сочетание параметров для алгоритма GradientBoostingClassifier = {gbc_best_param}')\n\nlr_best_param = lr_model.best_params_\nprint(f'Лучшее сочетание параметров для алгоритма LogisticRegression = {lr_best_param}')","execution_count":null,"outputs":[]},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":85},"colab_type":"code","id":"mykhQbmGBgGZ","outputId":"f8f2b613-3c55-4c30-9a18-071df0f14e50","trusted":true},"cell_type":"code","source":"# 3. \nrfc_pred = rfc_model.predict(x_test)\nsvc_pred = svc_model.predict(x_test)\ngbc_pred = gbc_model.predict(x_test)\nlr_pred = lr_model.predict(x_test)\n\naccuracy_rfc = rfc_model.score(x_test, y_test)\nprint(f'Accuracy rfc: {accuracy_rfc}')\n\naccuracy_svc = svc_model.score(x_test, y_test)\nprint(f'Accuracy svc: {accuracy_svc}')\n\naccuracy_gbc = gbc_model.score(x_test, y_test)\nprint(f'Accuracy gbc: {accuracy_gbc}')\n\naccuracy_lr = lr_model.score(x_test, y_test)\nprint(f'Accuracy lr: {accuracy_lr}')","execution_count":null,"outputs":[]},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":85},"colab_type":"code","id":"iJA3WiPoBgGb","outputId":"75442f41-25f4-4581-ecb4-6aaf589fe350","trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\n\nprint(f'Accuracy rfc : {roc_auc_score(y_test, rfc_pred)}')\nprint(f'Accuracy svc : {roc_auc_score(y_test, svc_pred)}')\nprint(f'Accuracy gbc : {roc_auc_score(y_test, gbc_pred)}')\nprint(f'Accuracy lr : {roc_auc_score(y_test, lr_pred)}')","execution_count":null,"outputs":[]},{"metadata":{"colab_type":"text","id":"nSzL39BKQ_h1"},"cell_type":"markdown","source":"### 3.\nС помощью GridSearchCV и указанных ниже параметров настрою мета-алгоритм на мета-признаках (использую 5-кратную валидацию и random_state=17 при определении алгоритма). Матрицу метапризнаков получу из предсказаний, полученных в предыдущем пункте на валидационных данных базовыми алгоритмами. Выведу лучшие параметры."},{"metadata":{"colab":{},"colab_type":"code","id":"XzgouulGQ_h4","trusted":true},"cell_type":"code","source":"xgb_params = {'n_estimators': range(10, 100, 5),\n              'eta': np.arange(0.1, 1., .1),\n              'min_child_weight': range(1, 10, 1),\n              'subsample': np.arange(0.1, 1., 0.2)}","execution_count":null,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"_FV0vZIeQ_h9","trusted":true},"cell_type":"code","source":"xgb = GridSearchCV(meta, xgb_params, cv=skf)","execution_count":null,"outputs":[]},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":340},"colab_type":"code","id":"kwVCuziYBgGp","outputId":"94efe888-7402-490b-db72-9d9ed359d0d0","trusted":true},"cell_type":"code","source":"models_pred = np.array([rfc_pred, svc_pred, gbc_pred, lr_pred])\n\nmeta_mtrx = np.empty((y_test.shape[0], len(models_pred)))\nfor n, model in enumerate(models_pred):\n    meta_mtrx[:, n] = model\n    \nxgb.fit(meta_mtrx, y_test)","execution_count":null,"outputs":[]},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":85},"colab_type":"code","id":"dB5M0G89BgGs","outputId":"e0876594-ae8a-4b09-b2f8-1c1a99eca564","trusted":true},"cell_type":"code","source":"# Выведу лучшие параметры.\nxgb.best_params_","execution_count":null,"outputs":[]},{"metadata":{"colab_type":"text","id":"yN1aFtouQ_iF"},"cell_type":"markdown","source":"### 4.\nПострою стекинг (использую 5-кратную кросс-валидацию) для всех моделей с наилучшими подобранными параметрами. В качестве тренировочных данных использую весь датасет train.csv, а в качестве тестовых - весь датасет test.csv. Сделаю прогноз мета-алгоритма для test.csv."},{"metadata":{"colab":{},"colab_type":"code","id":"rCJLmRQrBgGw","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_predict","execution_count":null,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"qgy66ttEBZuy","trusted":true},"cell_type":"code","source":"def stacking(models, meta_alg, data_train, targets_train, data_test, targets_test=None, random_state=None, test_size=None, cv=5):\n    \n    if test_size is None:\n        \n        meta_mtrx = np.empty((data_train.shape[0], len(models)))\n\n        for n, model in enumerate(models):\n            meta_mtrx[:, n] = cross_val_predict(model, data_train, targets_train, cv=cv, method='predict')\n            model.fit(data_train, targets_train)\n\n        meta_alg.fit(meta_mtrx, targets_train)\n\n        meta_mtrx_test = np.empty((data_test.shape[0], len(models)))\n\n        for n, model in enumerate(models):\n            meta_mtrx_test[:, n] = model.predict(data_test)\n\n    elif test_size > 0 and test_size < 1:\n        \n        x_train, x_test, y_train, y_test = train_test_split(data_train, targets_train, test_size=test_size, random_state=random_state)\n        \n        meta_mtrx = np.empty((x_test.shape[0], len(models)))\n\n        for n, model in enumerate(models):\n            model.fit(x_train, y_train)\n            meta_mtrx[:, n] = model.predict(x_test)\n\n        meta_mtrx_pred = meta_alg.fit(meta_mtrx, y_test)\n\n        meta_mtrx_test = np.empty((data_test.shape[0], len(models)))\n\n        for n, model in enumerate(models):\n            meta_mtrx_test[:, n] = model.predict(data_test)\n\n    else:\n        raise ValueError(\"test_size must be between 0 and 1\")\n        \n    return meta_alg, meta_mtrx_test","execution_count":null,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"n2srQdMeBfIg","trusted":true},"cell_type":"code","source":"rfc = RandomForestClassifier(min_samples_leaf=3, n_estimators=60, random_state=17)\nsvc = SVC(C=0.1, degree=2, gamma=1e-05, kernel='linear', random_state=17)\ngbc = GradientBoostingClassifier(learning_rate=0.1, random_state=17)\nlr = LogisticRegression(C=0.8999999999999999, solver='lbfgs', random_state=17)\n\nxgb = XGBClassifier(eta=0.30000000000000004, min_child_weight=1, n_estimators=20, subsample=0.1)\n\nmodels = [rfc, lr, svc, gbc] # список базовых алгоритмов с наилучшими подобранными параметрами","execution_count":null,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"EBNXrVeDBmup","trusted":true},"cell_type":"code","source":"# тест на тренировочных данных\nmeta_alg, meta_mtrx_test = stacking(models = models, \n                                    meta_alg = xgb, \n                                    data_train = x_train,\n                                    targets_train = y_train,\n                                    data_test = x_test,\n                                    targets_test = y_test, \n                                    test_size = 0.3)","execution_count":null,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"I_5gLCBSBmxv","trusted":true},"cell_type":"code","source":"meta_alg_predict = meta_alg.predict(meta_mtrx_test)","execution_count":null,"outputs":[]},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"colab_type":"code","id":"1-9GQwBBBm1X","outputId":"02cf6566-1f0b-4a81-aa03-89c5424cba2d","trusted":true},"cell_type":"code","source":"print(f'Stacking AUC: {roc_auc_score(y_test, meta_alg_predict)}')","execution_count":null,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"KI0R-ifdQ_iH","trusted":true},"cell_type":"code","source":"# В качестве тренировочных данных использую весь датасет train.csv, а в качестве тестовых - весь датасет test.csv. \n# Сделаю прогноз мета-алгоритма для test.csv.\nmeta_alg, meta_mtrx_test = stacking(models = models, \n                                    meta_alg = xgb,\n                                    data_train = data,\n                                    targets_train = targets,\n                                    data_test = test,\n                                    targets_test = None, \n                                    test_size = 0.3)","execution_count":null,"outputs":[]},{"metadata":{"colab_type":"text","id":"NOZxNFPxQ_iM"},"cell_type":"markdown","source":"### 5.\nС помощью нижеприведенной функции сформирую файл посылки для соревнования и отправлю на Kaggle."},{"metadata":{"colab":{},"colab_type":"code","id":"qOvbS_fPQ_iO","trusted":true},"cell_type":"code","source":"def write_to_submission_file(meta_predictions, PassengerID, out_file='Submission.csv', columns=['PassengerID', 'Survived']):\n    predicted_df = pd.DataFrame(np.array([PassengerId, meta_predictions]).T, columns=columns)\n    predicted_df.to_csv(out_file, index=False)","execution_count":null,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"TkjG_uRsQ_iV","trusted":true},"cell_type":"code","source":"# ваш код\npredictions = meta_alg.predict(meta_mtrx_test)\nPassengerID = PassengerId\n\nwrite_to_submission_file(predictions, PassengerID)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions","execution_count":null,"outputs":[]},{"metadata":{"colab_type":"text","id":"28lOtXXQQ_ib"},"cell_type":"markdown","source":"### 6.\nРезультат score, полученного на соревновании"},{"metadata":{"colab_type":"text","id":"InoT842SQ_id"},"cell_type":"markdown","source":"Ваш ответ: 0.79425"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"accelerator":"TPU","colab":{"collapsed_sections":[],"name":"jun_ml_kaggle_hw.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":4}